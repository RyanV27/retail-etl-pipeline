{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7ab69aa-3c78-4626-b278-4b75ae6719ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./extractor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef392bf1-1ccc-4332-bb2b-141f175db730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./transformer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a223a3f-b64c-4cf6-9594-34925e14c1f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./loader\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b93a1ec-6fc1-4ba8-969b-3406dd3afda5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f492ea67-81b0-4093-9ac7-c69171e2dbd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"AppleDataAnalysis\").getOrCreate()\n",
    "\n",
    "# Reading the transactions data from the csv file stored in DBFS\n",
    "input_df = spark.read.format(\"csv\").option(\"header\", True).load(\"dbfs:/FileStore/tables/Transaction_Updated.csv\")\n",
    "\n",
    "input_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c8d1e7-9cac-4bbe-b177-e42ec78caeda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The main code to start our program's run\n",
    "\n",
    "class AirpodsAfteriPhoneWorkflow():\n",
    "    \"\"\"\n",
    "    ETL Pipeline to genearate of all customers who bought AirPods just after buying iPhone.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def runner(self):\n",
    "        # ETL Pipeline\n",
    "        # 1. Extract all data from different sources\n",
    "        inputDFs = AirpodsAfteriPhoneExtractor().extract() \n",
    "\n",
    "        # 2. Tranform logic implementation to find customers who bought AirPods right after \n",
    "        # buying an iPhone.\n",
    "        firstTransformedDF = AirpodsAfteriPhoneTransformer().transform(\n",
    "            inputDFs = inputDFs\n",
    "        )\n",
    "\n",
    "        # 3. Load all the data to different sinks.\n",
    "        AirpodsAfteriPhoneLoader(transformedDF = firstTransformedDF).sink()\n",
    "\n",
    "        # Testing to see if data was loaded correctly\n",
    "        load_test_df = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/apple_analysis/output/airpodsAfteriPhone\")\n",
    "        print(\"Stored data:-\")\n",
    "        load_test_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14f6cb34-07f2-483c-acba-99c631d5e12d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The main code to start our program's run\n",
    "\n",
    "class OnlyAirpodsAndiPhoneWorkflow():\n",
    "    \"\"\"\n",
    "    ETL Pipeline to genearate of all customers who bought AirPods just after buying iPhone.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def runner(self):\n",
    "        # ETL Pipeline\n",
    "        # 1. Extract all data from different sources using the same extractor as before since the required dataframes\n",
    "        # are the same.\n",
    "        inputDFs = AirpodsAfteriPhoneExtractor().extract() \n",
    "\n",
    "        # 2. Tranform logic implementation to find customers who bought AirPods right after \n",
    "        # buying an iPhone.\n",
    "        transformedDF = OnlyAirpodsAndiPhoneTransformer().transform(\n",
    "            inputDFs = inputDFs\n",
    "        )\n",
    "\n",
    "        # 3. Load all the data to different sinks.\n",
    "        OnlyAirpodsAndiPhoneLoader(transformedDF = transformedDF).sink()\n",
    "\n",
    "        # Testing to see if data was loaded correctly\n",
    "        load_test_df = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/apple_analysis/output/airpodsOnlyiPhone\")\n",
    "        print(\"Stored data from Datalake:-\")\n",
    "        load_test_df.show()\n",
    "\n",
    "        load_test_df = spark.read.format(\"delta\").table(\"default.onlyAirpodsAndiPhone\")\n",
    "        print(\"Stored data from Delta table:-\")\n",
    "        load_test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa65ed83-c9aa-42d3-80fd-b99836d35d7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers dataframe: -\n+-----------+-------------+----------+--------+\n|customer_id|customer_name| join_date|location|\n+-----------+-------------+----------+--------+\n|        105|          Eva|2022-01-01|    Ohio|\n|        106|        Frank|2022-02-01|  Nevada|\n|        107|        Grace|2022-03-01|Colorado|\n|        108|        Henry|2022-04-01|    Utah|\n+-----------+-------------+----------+--------+\n\nTransactions dataframe before the tranform: -\n+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\nProducts bought by each customer:\n+-----------+--------------------+\n|customer_id|            products|\n+-----------+--------------------+\n|        107|   [AirPods, iPhone]|\n|        108|   [AirPods, iPhone]|\n|        106|[AirPods, iPhone,...|\n|        105|[AirPods, iPhone,...|\n+-----------+--------------------+\n\nShowing the customers who bought 'AirPods' and 'iPhone' only: -\n+-----------+-----------------+\n|customer_id|         products|\n+-----------+-----------------+\n|        107|[AirPods, iPhone]|\n|        108|[AirPods, iPhone]|\n+-----------+-----------------+\n\nStored data from Datalake:-\n+-----------+-------------+--------+\n|customer_id|customer_name|location|\n+-----------+-------------+--------+\n|        107|        Grace|Colorado|\n|        108|        Henry|    Utah|\n+-----------+-------------+--------+\n\nStored data from Delta table:-\n+-----------+-------------+--------+\n|customer_id|customer_name|location|\n+-----------+-------------+--------+\n|        107|        Grace|Colorado|\n|        108|        Henry|    Utah|\n+-----------+-------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# In place of a scheduler like Apache Airflow we are definning a class to call each use cases's runner\n",
    "class WorkflowRunner():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def runner(self):\n",
    "        if(self.name == \"AirpodsAfteriPhoneWorkflow\"):\n",
    "            return(AirpodsAfteriPhoneWorkflow().runner())\n",
    "        elif(self.name == \"OnlyAirpodsAndiPhoneWorkflow\"):\n",
    "            return(OnlyAirpodsAndiPhoneWorkflow().runner())\n",
    "        else:\n",
    "            raise ValueError(f\"Not implemented for \\'{self.name}\\'\")\n",
    "        \n",
    "\n",
    "name = \"OnlyAirpodsAndiPhoneWorkflow\"\n",
    "WorkflowRunner(name).runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be5eec25-91c4-438a-aeff-3b4659a38110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.rm(\"dbfs:/FileStore/tables/apple_analysis/output/airpodsOnlyiPhone\", recurse=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "apple_analysis_workflow",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
